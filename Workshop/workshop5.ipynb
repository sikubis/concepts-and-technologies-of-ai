{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "0hryHAJjmLZL"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "\n",
        "# 3.1 Implementation from Scratch\n",
        "# 3.1.1 Data Understanding, Analysis and Preparations:\n",
        "\n",
        "# To Do Task:\n",
        "\n",
        "def train_test_split(X,Y,test_size=0.3,random_state=42):\n",
        "  np.random.seed(random_state)\n",
        "  indices = np.arange(X.shape[0])\n",
        "  np.random.shuffle(indices)\n",
        "  test_split_size = int(len(X)*test_size)\n",
        "  train_indices=indices[:test_split_size]\n",
        "  test_indices = indices[test_split_size:]\n",
        "  X_train,X_test=X[train_indices],X[test_indices]\n",
        "  Y_train,Y_test=Y[train_indices],Y[test_indices]\n",
        "  return X_train,X_test,Y_train,Y_test"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def cost_function(X, Y, W):\n",
        "  \"\"\" Parameters:\n",
        "  This function finds the Mean Square Error.\n",
        "  Input parameters:\n",
        "  X: Feature Matrix\n",
        "  Y: Target Matrix\n",
        "  W: Weight Matrix\n",
        "  Output Parameters:\n",
        "  cost: accumulated mean square error.\n",
        "  \"\"\"\n",
        "  # Your code here:\n",
        "\n",
        "  Y_pred = np.matmul(W,np.transpose(X))\n",
        "  sum_error = [ (Y_pred[i] - Y[i])**2 for i in range(len(Y))]\n",
        "  cost = sum(sum_error) / 2*len(Y)\n",
        "  return cost"
      ],
      "metadata": {
        "id": "35pf1uzrnyCX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "def gradient_descent(X, Y, W, alpha, iterations):\n",
        "  \"\"\"\n",
        "  Perform gradient descent to optimize the parameters of a linear regression model.\n",
        "  Parameters:\n",
        "  X (numpy.ndarray): Feature matrix (m x n).\n",
        "  Y (numpy.ndarray): Target vector (m x 1).\n",
        "  W (numpy.ndarray): Initial guess for parameters (n x 1).\n",
        "  alpha (float): Learning rate.\n",
        "  iterations (int): Number of iterations for gradient descent.\n",
        "  Returns:\n",
        "  tuple: A tuple containing the final optimized parameters (W_update) and the history of cost values.\n",
        "  W_update (numpy.ndarray): Updated parameters (n x 1).\n",
        "  cost_history (list): History of cost values over iterations.\n",
        "  \"\"\"\n",
        "  # Initialize cost history\n",
        "  cost_history = [0] * iterations\n",
        "  # Number of samples\n",
        "  m = len(Y)\n",
        "  for iteration in range(iterations):\n",
        "  # Step 1: Hypothesis Values\n",
        "    Y_pred = np.matmul(W,np.transpose(X))# Your Code Here\n",
        "  # Step 2: Difference between Hypothesis and Actual Y\n",
        "    loss = Y_pred - Y# Your Code Here\n",
        "  # Step 3: Gradient Calculation\n",
        "    dw = sum(np.dot(loss,X))/m# Your Code Here\n",
        "  # Step 4: Updating Values of W using Gradient\n",
        "    W_update = W - alpha * dw# Your Code Here\n",
        "    W = W_update\n",
        "  # Step 5: New Cost Value\n",
        "    cost = cost_function(X, Y, W_update)\n",
        "    cost_history[iteration] = cost\n",
        "  return W_update, cost_history"
      ],
      "metadata": {
        "id": "ZkPpck9un9YU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Model Evaluation - RMSE\n",
        "def rmse(Y, Y_pred):\n",
        "  \"\"\"\n",
        "  This Function calculates the Root Mean Squres.\n",
        "  Input Arguments:\n",
        "  Y: Array of actual(Target) Dependent Varaibles.\n",
        "  Y_pred: Array of predeicted Dependent Varaibles.\n",
        "  Output Arguments:\n",
        "  rmse: Root Mean Square.\n",
        "  \"\"\"\n",
        "  loss = (Y - Y_pred)**2\n",
        "  rmse = np.sqrt(1/len(Y)*sum(loss))# Your Code Here\n",
        "  return rmse"
      ],
      "metadata": {
        "id": "xf3dwOHCoJ7F"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Model Evaluation - R2\n",
        "def r2(Y, Y_pred):\n",
        "  \"\"\"\n",
        "  This Function calculates the R Squared Error.\n",
        "  Input Arguments:\n",
        "  Y: Array of actual(Target) Dependent Varaibles.\n",
        "  Y_pred: Array of predeicted Dependent Varaibles.\n",
        "  Output Arguments:\n",
        "  rsquared: R Squared Error.\n",
        "  \"\"\"\n",
        "  mean_y = np.mean(Y)\n",
        "  ss_tot = (Y-mean_y)**2# Your Code Here\n",
        "  ss_res = (Y-Y_pred)**2# Your Code Here\n",
        "  r2 = 1 - sum(ss_res)/sum(ss_tot)# Your Code Here\n",
        "  return r2"
      ],
      "metadata": {
        "id": "Xxj_2Ej-oPIp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def main():\n",
        "  # Step 1: Load the dataset\n",
        "  data = pd.read_csv('student.csv')\n",
        "  # Step 2: Split the data into features (X) and target (Y)\n",
        "  X = data[['Math', 'Reading']].values # Features: Math and Reading marks\n",
        "  Y = data['Writing'].values # Target: Writing marks\n",
        "  # Step 3: Split the data into training and test sets (80% train, 20% test)\n",
        "  X_train, X_test, Y_train, Y_test = train_test_split(X, Y, test_size=0.2, random_state=42)\n",
        "  # Step 4: Initialize weights (W) to zeros, learning rate and number of iterations\n",
        "  W = np.zeros(X_train.shape[1]) # Initialize weights\n",
        "  alpha = 0.0001 # Learning rate\n",
        "  iterations = 1000 # Number of iterations for gradient descent\n",
        "  # Step 5: Perform Gradient Descent\n",
        "  W_optimal, cost_history = gradient_descent(X_train, Y_train, W, alpha, iterations)\n",
        "  # Step 6: Make predictions on the test set\n",
        "  Y_pred = np.dot(X_test, W_optimal)\n",
        "  # Step 7: Evaluate the model using RMSE and R-Squared\n",
        "  model_rmse = rmse(Y_test, Y_pred)\n",
        "  model_r2 = r2(Y_test, Y_pred)\n",
        "  # Step 8: Output the results\n",
        "  print(\"Final Weights:\", W_optimal)\n",
        "  print(\"Cost History (First 10 iterations):\", cost_history[:10])\n",
        "  print(\"RMSE on Test Set:\", model_rmse)\n",
        "  print(\"R-Squared on Test Set:\", model_r2)\n",
        "# Execute the main function\n",
        "if __name__ == \"__main__\":\n",
        "  main()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "j1mJ3pRNofLr",
        "outputId": "fc3bea94-8cc1-4019-a88a-592583dbb391"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Final Weights: [0.50076643 0.50076643]\n",
            "Cost History (First 10 iterations): [np.float64(89125947.18363665), np.float64(80609561.13062757), np.float64(72913584.29117385), np.float64(65958984.16563353), np.float64(59674341.69397058), np.float64(53995117.83010217), np.float64(48862990.76935666), np.float64(44225257.02281351), np.float64(40034290.1879625), np.float64(36247051.85762219)]\n",
            "RMSE on Test Set: 5.90576130811891\n",
            "R-Squared on Test Set: 0.8466800920643259\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "ArI8JswDooXd"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}